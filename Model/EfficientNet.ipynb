{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2200a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# dataset and transformation\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5144b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "data_test = []\n",
    "\n",
    "def classTrainData(): # set data_train\n",
    "    path_dir = './res_data/train'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    for data in file_list:\n",
    "      temp = np.load('./res_data/train/' + data)\n",
    "      tempData = torch.Tensor(temp)\n",
    "      if \"AD\" in data:\n",
    "        temp = (tempData, 0)\n",
    "        data_train.append(temp)\n",
    "      elif \"NM\" in data:\n",
    "        temp = (tempData, 1)\n",
    "        data_train.append(temp)\n",
    "      elif \"PD\" in data:\n",
    "        temp = (tempData, 2)\n",
    "        data_train.append(temp)\n",
    "\n",
    "def classTestData(): # set data_train\n",
    "    path_dir = './res_data/test'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    for data in file_list:\n",
    "      temp = np.load('./res_data/test/' + data)\n",
    "      tempData = torch.Tensor(temp)\n",
    "      if \"AD\" in data:\n",
    "        temp = (tempData, 0)\n",
    "        data_test.append(temp)\n",
    "      elif \"NM\" in data:\n",
    "        temp = (tempData, 1)\n",
    "        data_test.append(temp)\n",
    "      elif \"PD\" in data:\n",
    "        temp = (tempData, 2)\n",
    "        data_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b94bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classTrainData()\n",
    "classTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "957c2095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947a701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation\n",
    "transformation = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize(224)\n",
    "])\n",
    "\n",
    "# apply transformation to dataset\n",
    "for i in range(0, len(data_train)):\n",
    "    data_train[i][0].transform = transformation\n",
    "for i in range(0, len(data_test):\n",
    "    data_test[i][0].transform = transformation\n",
    "\n",
    "# make dataloader\n",
    "train_dl = DataLoader(data_train, batch_size=10, shuffle=True)\n",
    "val_dl = DataLoader(data_test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646928c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([4, 4, 224, 224])\n",
      "output size: torch.Size([4, 56, 1, 1])\n",
      "output size: torch.Size([4, 16, 24, 24]) Stochastic depth: tensor(False)\n",
      "output size: torch.Size([4, 16, 24, 24]) Stochastic depth: tensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 4, 224, 224)\n",
    "    model = Swish()\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())\n",
    "    \n",
    "    # SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels * r),\n",
    "            Swish(),\n",
    "            nn.Linear(in_channels * r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 56, 17, 17)\n",
    "    model = SEBlock(x.size(1))\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())\n",
    "    \n",
    "class MBConv(nn.Module):\n",
    "    expand = 6\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first MBConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 16, 24, 24)\n",
    "    model = MBConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])\n",
    "    \n",
    "class SepConv(nn.Module):\n",
    "    expand = 1\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first SepConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 16, 24, 24)\n",
    "    model = SepConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    # stochastic depth check\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])\n",
    "    \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "\n",
    "        channels = [int(x*width) for x in channels]\n",
    "        repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "        # stochastic depth\n",
    "        if stochastic_depth:\n",
    "            self.p = p\n",
    "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "        else:\n",
    "            self.p = 1\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "        # efficient net\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(4, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(channels[8], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "        strides = [stride] + [1] * (repeats - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "            in_channels = out_channels\n",
    "            self.p -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b1(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b2(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b3(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b4(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b5(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b6(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "def efficientnet_b7(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.randn(4, 4, 224, 224).to(device)\n",
    "    model = efficientnet_b4().to(device)\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea6e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# define loss function, optimizer, lr_scheduler\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "# get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# calculate the metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss_b = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item(), metric_b\n",
    "\n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights=params['path2weights']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf09b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':60,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# check the directory to save weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f80007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 1.068896, val loss: 1.202285, accuracy: 42.73, time: 2.6703 min\n",
      "----------\n",
      "Epoch 1/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 1.003090, val loss: 0.952421, accuracy: 56.33, time: 4.8100 min\n",
      "----------\n",
      "Epoch 2/59, current lr= 0.01\n",
      "train loss: 0.977454, val loss: 0.958217, accuracy: 53.12, time: 7.0869 min\n",
      "----------\n",
      "Epoch 3/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 0.968862, val loss: 0.951223, accuracy: 55.55, time: 9.2494 min\n",
      "----------\n",
      "Epoch 4/59, current lr= 0.01\n",
      "train loss: 0.960906, val loss: 0.967320, accuracy: 50.47, time: 11.3508 min\n",
      "----------\n",
      "Epoch 5/59, current lr= 0.01\n",
      "train loss: 0.963671, val loss: 0.966151, accuracy: 54.69, time: 13.6787 min\n",
      "----------\n",
      "Epoch 6/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 0.957144, val loss: 0.921427, accuracy: 56.41, time: 16.0082 min\n",
      "----------\n",
      "Epoch 7/59, current lr= 0.01\n",
      "train loss: 0.949170, val loss: 0.930315, accuracy: 57.27, time: 18.3527 min\n",
      "----------\n",
      "Epoch 8/59, current lr= 0.01\n",
      "train loss: 0.939951, val loss: 0.955008, accuracy: 52.03, time: 20.6719 min\n",
      "----------\n",
      "Epoch 9/59, current lr= 0.01\n",
      "train loss: 0.920760, val loss: 0.935719, accuracy: 58.28, time: 22.9598 min\n",
      "----------\n",
      "Epoch 10/59, current lr= 0.01\n",
      "train loss: 0.927271, val loss: 1.154544, accuracy: 44.53, time: 25.1348 min\n",
      "----------\n",
      "Epoch 11/59, current lr= 0.01\n",
      "train loss: 0.913741, val loss: 1.509895, accuracy: 53.05, time: 27.2600 min\n",
      "----------\n",
      "Epoch 12/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 0.903018, val loss: 0.879460, accuracy: 58.67, time: 29.3700 min\n",
      "----------\n",
      "Epoch 13/59, current lr= 0.01\n",
      "train loss: 0.892686, val loss: 1.151115, accuracy: 53.75, time: 31.5091 min\n",
      "----------\n",
      "Epoch 14/59, current lr= 0.01\n",
      "train loss: 0.894349, val loss: 1.130860, accuracy: 45.39, time: 33.5608 min\n",
      "----------\n",
      "Epoch 15/59, current lr= 0.01\n",
      "train loss: 0.882600, val loss: 0.967775, accuracy: 57.66, time: 35.6326 min\n",
      "----------\n",
      "Epoch 16/59, current lr= 0.01\n",
      "train loss: 0.859485, val loss: 0.884502, accuracy: 60.70, time: 37.6800 min\n",
      "----------\n",
      "Epoch 17/59, current lr= 0.01\n",
      "train loss: 0.859004, val loss: 1.084043, accuracy: 49.84, time: 39.7112 min\n",
      "----------\n",
      "Epoch 18/59, current lr= 0.01\n",
      "train loss: 0.832279, val loss: 1.110710, accuracy: 47.81, time: 41.7297 min\n",
      "----------\n",
      "Epoch 19/59, current lr= 0.01\n",
      "train loss: 0.838111, val loss: 1.093511, accuracy: 46.72, time: 43.7402 min\n",
      "----------\n",
      "Epoch 20/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 0.816124, val loss: 0.838528, accuracy: 62.27, time: 45.8013 min\n",
      "----------\n",
      "Epoch 21/59, current lr= 0.01\n",
      "train loss: 0.798964, val loss: 0.857476, accuracy: 60.39, time: 47.8506 min\n",
      "----------\n",
      "Epoch 22/59, current lr= 0.01\n",
      "train loss: 0.796932, val loss: 1.314555, accuracy: 36.64, time: 49.9133 min\n",
      "----------\n",
      "Epoch 23/59, current lr= 0.01\n",
      "train loss: 0.796502, val loss: 1.340996, accuracy: 49.53, time: 51.9327 min\n",
      "----------\n",
      "Epoch 24/59, current lr= 0.01\n",
      "train loss: 0.775962, val loss: 0.932004, accuracy: 54.14, time: 53.9438 min\n",
      "----------\n",
      "Epoch 25/59, current lr= 0.01\n",
      "train loss: 0.751926, val loss: 0.880336, accuracy: 63.36, time: 56.0031 min\n",
      "----------\n",
      "Epoch 26/59, current lr= 0.01\n",
      "train loss: 0.749023, val loss: 1.910175, accuracy: 58.59, time: 58.0856 min\n",
      "----------\n",
      "Epoch 27/59, current lr= 0.01\n",
      "train loss: 0.730046, val loss: 1.171898, accuracy: 54.45, time: 60.1833 min\n",
      "----------\n",
      "Epoch 28/59, current lr= 0.01\n",
      "train loss: 0.732511, val loss: 1.254813, accuracy: 48.83, time: 62.2486 min\n",
      "----------\n",
      "Epoch 29/59, current lr= 0.01\n",
      "train loss: 0.702978, val loss: 1.315848, accuracy: 49.22, time: 64.3007 min\n",
      "----------\n",
      "Epoch 30/59, current lr= 0.01\n",
      "train loss: 0.680989, val loss: 0.879121, accuracy: 61.33, time: 66.3229 min\n",
      "----------\n",
      "Epoch 31/59, current lr= 0.01\n",
      "Copied best model weights!\n",
      "train loss: 0.691571, val loss: 0.823824, accuracy: 64.69, time: 68.4247 min\n",
      "----------\n",
      "Epoch 32/59, current lr= 0.01\n",
      "train loss: 0.642602, val loss: 0.884532, accuracy: 63.98, time: 70.4972 min\n",
      "----------\n",
      "Epoch 33/59, current lr= 0.01\n",
      "train loss: 0.641045, val loss: 1.053604, accuracy: 56.41, time: 72.5545 min\n",
      "----------\n",
      "Epoch 34/59, current lr= 0.01\n",
      "train loss: 0.623225, val loss: 1.250143, accuracy: 48.98, time: 74.6091 min\n",
      "----------\n",
      "Epoch 35/59, current lr= 0.01\n",
      "train loss: 0.601195, val loss: 0.847263, accuracy: 64.30, time: 76.6092 min\n",
      "----------\n",
      "Epoch 36/59, current lr= 0.01\n",
      "train loss: 0.573752, val loss: 0.935875, accuracy: 62.97, time: 78.6211 min\n",
      "----------\n",
      "Epoch 37/59, current lr= 0.01\n",
      "train loss: 0.589959, val loss: 0.896243, accuracy: 63.05, time: 80.6539 min\n",
      "----------\n",
      "Epoch 38/59, current lr= 0.01\n",
      "train loss: 0.527001, val loss: 0.956900, accuracy: 64.45, time: 82.6853 min\n",
      "----------\n",
      "Epoch 39/59, current lr= 0.01\n",
      "train loss: 0.534846, val loss: 1.041990, accuracy: 57.34, time: 84.6986 min\n",
      "----------\n",
      "Epoch 40/59, current lr= 0.01\n",
      "train loss: 0.493995, val loss: 0.928565, accuracy: 64.53, time: 86.7516 min\n",
      "----------\n",
      "Epoch 41/59, current lr= 0.01\n",
      "train loss: 0.502040, val loss: 1.020143, accuracy: 63.05, time: 88.8553 min\n",
      "----------\n",
      "Epoch 42/59, current lr= 0.01\n",
      "Loading best model weights!\n",
      "train loss: 0.449281, val loss: 1.226977, accuracy: 61.17, time: 90.9094 min\n",
      "----------\n",
      "Epoch 43/59, current lr= 0.001\n",
      "train loss: 0.566069, val loss: 0.863987, accuracy: 65.00, time: 92.9363 min\n",
      "----------\n",
      "Epoch 44/59, current lr= 0.001\n",
      "train loss: 0.522411, val loss: 1.332266, accuracy: 51.88, time: 95.0680 min\n",
      "----------\n",
      "Epoch 45/59, current lr= 0.001\n",
      "train loss: 0.502815, val loss: 0.880893, accuracy: 63.36, time: 97.1746 min\n",
      "----------\n",
      "Epoch 46/59, current lr= 0.001\n",
      "train loss: 0.479902, val loss: 0.971857, accuracy: 63.44, time: 99.2281 min\n",
      "----------\n",
      "Epoch 47/59, current lr= 0.001\n",
      "train loss: 0.468434, val loss: 0.926733, accuracy: 65.16, time: 101.2929 min\n",
      "----------\n",
      "Epoch 48/59, current lr= 0.001\n",
      "train loss: 0.446910, val loss: 0.946782, accuracy: 64.77, time: 103.3146 min\n",
      "----------\n",
      "Epoch 49/59, current lr= 0.001\n",
      "train loss: 0.442477, val loss: 0.925459, accuracy: 63.44, time: 105.4013 min\n",
      "----------\n",
      "Epoch 50/59, current lr= 0.001\n",
      "train loss: 0.431093, val loss: 0.948157, accuracy: 64.84, time: 107.4700 min\n",
      "----------\n",
      "Epoch 51/59, current lr= 0.001\n",
      "train loss: 0.426557, val loss: 1.041806, accuracy: 65.70, time: 109.4914 min\n",
      "----------\n",
      "Epoch 52/59, current lr= 0.001\n",
      "train loss: 0.418706, val loss: 1.019195, accuracy: 63.83, time: 111.5441 min\n",
      "----------\n",
      "Epoch 53/59, current lr= 0.001\n",
      "Loading best model weights!\n",
      "train loss: 0.404908, val loss: 0.984666, accuracy: 64.14, time: 113.5804 min\n",
      "----------\n",
      "Epoch 54/59, current lr= 0.0001\n",
      "train loss: 0.602062, val loss: 0.876759, accuracy: 62.03, time: 115.6148 min\n",
      "----------\n",
      "Epoch 55/59, current lr= 0.0001\n",
      "Copied best model weights!\n",
      "train loss: 0.567402, val loss: 0.811445, accuracy: 65.70, time: 117.6755 min\n",
      "----------\n",
      "Epoch 56/59, current lr= 0.0001\n",
      "train loss: 0.557663, val loss: 1.094414, accuracy: 61.64, time: 119.7689 min\n",
      "----------\n",
      "Epoch 57/59, current lr= 0.0001\n",
      "train loss: 0.546005, val loss: 0.926132, accuracy: 60.86, time: 121.8061 min\n",
      "----------\n",
      "Epoch 58/59, current lr= 0.0001\n",
      "train loss: 0.529475, val loss: 0.818099, accuracy: 65.08, time: 123.8833 min\n",
      "----------\n",
      "Epoch 59/59, current lr= 0.0001\n",
      "train loss: 0.518598, val loss: 0.852785, accuracy: 64.22, time: 125.9872 min\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2277de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 16 12\n",
      "16 18 6\n",
      "11 22 7\n",
      "12 16 12\n",
      "16 16 8\n",
      "18 18 4\n",
      "11 23 6\n",
      "10 17 13\n",
      "16 18 6\n",
      "14 20 6\n",
      "13 13 14\n",
      "19 12 9\n",
      "17 16 7\n",
      "17 12 11\n",
      "16 19 5\n",
      "12 18 10\n",
      "18 17 5\n",
      "13 19 8\n",
      "13 21 6\n",
      "7 26 7\n",
      "17 17 6\n",
      "10 18 12\n",
      "8 27 5\n",
      "17 14 9\n",
      "13 19 8\n",
      "14 18 8\n",
      "13 14 13\n",
      "17 13 10\n",
      "14 14 12\n",
      "14 18 8\n",
      "17 15 8\n",
      "11 16 13\n",
      "9 23 8\n",
      "12 18 10\n",
      "13 17 10\n",
      "13 18 9\n",
      "9 25 6\n",
      "18 13 9\n",
      "8 23 9\n",
      "14 19 7\n"
     ]
    }
   ],
   "source": [
    "# No Label Data\n",
    "path_dir = './PPTD'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "real_data_test = [] # Tensor 로 변환된 No Label Data\n",
    "for data in file_list:\n",
    "    temp = np.load('./PPTD/' + data)\n",
    "    tempData = torch.Tensor(temp)\n",
    "    real_data_test.append(tempData)\n",
    "    \n",
    "\n",
    "data_loader = DataLoader(dataset = real_data_test, batch_size=40, shuffle = False, drop_last = True)\n",
    "network = model\n",
    "\n",
    "with torch.no_grad(): # Model 의 Back Propagation 을 막음\n",
    "    for img in data_loader:\n",
    "        img = img.to(device)\n",
    "        pred = network(img)\n",
    "        tmp = torch.argmax(pred, 1)\n",
    "       \n",
    "        cntAD = 0 # 1명분 40개의 발화 데이터 Output 중 AD의 개수\n",
    "        cntNM = 0 # 1명분 40개의 발화 데이터 Output 중 NM의 개수\n",
    "        cntPD = 0 # 1명분 40개의 발화 데이터 Output 중 PD의 개수\n",
    "\n",
    "        for i in range(0, 40):\n",
    "            if(tmp[i] == 0): # AD 일 떄 \n",
    "                cntAD = cntAD + 1\n",
    "            elif(tmp[i] == 1): # NM 일 때\n",
    "                cntNM = cntNM + 1\n",
    "            elif(tmp[i] == 2): # PD 일 떄 \n",
    "                cntPD = cntPD + 1\n",
    "        \n",
    "        print(cntAD, cntNM, cntPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b486a72a-577a-45b2-bba4-70be505b0638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 16 7\n",
      "13 17 10\n",
      "7 24 9\n",
      "7 19 14\n",
      "8 21 11\n",
      "8 21 11\n",
      "7 22 11\n",
      "9 22 9\n",
      "9 23 8\n",
      "10 22 8\n",
      "12 20 8\n",
      "12 18 10\n",
      "13 19 8\n",
      "7 20 13\n",
      "7 23 10\n",
      "11 22 7\n",
      "12 24 4\n",
      "13 19 8\n",
      "11 21 8\n",
      "14 15 11\n",
      "7 25 8\n",
      "12 21 7\n",
      "8 21 11\n",
      "11 19 10\n",
      "12 21 7\n",
      "8 19 13\n",
      "7 24 9\n",
      "15 22 3\n",
      "16 15 9\n",
      "8 22 10\n",
      "9 22 9\n",
      "5 25 10\n"
     ]
    }
   ],
   "source": [
    "network = model\n",
    "\n",
    "# val_ds train Data를 8 : 2 로 나눈 데이터중 validation Dataset\n",
    "data_loader = DataLoader(dataset = val_ds, batch_size=40, shuffle = False, drop_last = True)\n",
    "\n",
    "# Label 된 Data를 통해 No Label Data의 기준을 잡음\n",
    "with torch.no_grad():\n",
    "    for img, label in data_loader:\n",
    "        img = img.to(device)\n",
    "        pred = network(img)\n",
    "        tmp = torch.argmax(pred, 1)\n",
    "       \n",
    "        cntAD = 0\n",
    "        cntNM = 0\n",
    "        cntPD = 0\n",
    "\n",
    "        for i in range(0, 40):\n",
    "            if(tmp[i] == 0): # AD 일 떄 \n",
    "                cntAD = cntAD + 1\n",
    "            elif(tmp[i] == 1): # NM 일 때\n",
    "                cntNM = cntNM + 1\n",
    "            elif(tmp[i] == 2): # PD 일 떄 \n",
    "                cntPD = cntPD + 1\n",
    "        \n",
    "        print(cntAD, cntNM, cntPD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
