{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# dataset and transformation\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fa754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "data_test = []\n",
    "\n",
    "def classTrainData(): # set data_train\n",
    "    path_dir = './res_data/train/2'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    for data in file_list:\n",
    "      temp = np.load('./res_data/train/2/' + data)\n",
    "      tempData = torch.Tensor(temp)\n",
    "      if \"AD\" in data:\n",
    "        temp = (tempData, 0)\n",
    "        data_train.append(temp)\n",
    "      elif \"NM\" in data:\n",
    "        temp = (tempData, 1)\n",
    "        data_train.append(temp)\n",
    "      elif \"PD\" in data:\n",
    "        temp = (tempData, 2)\n",
    "        data_train.append(temp)\n",
    "\n",
    "def classTestData(): # set data_train\n",
    "    path_dir = './res_data/test/2'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    for data in file_list:\n",
    "      temp = np.load('./res_data/test/2/' + data)\n",
    "      tempData = torch.Tensor(temp)\n",
    "      if \"AD\" in data:\n",
    "        temp = (tempData, 0)\n",
    "        data_test.append(temp)\n",
    "      elif \"NM\" in data:\n",
    "        temp = (tempData, 1)\n",
    "        data_test.append(temp)\n",
    "      elif \"PD\" in data:\n",
    "        temp = (tempData, 2)\n",
    "        data_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classTrainData()\n",
    "classTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a23612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation\n",
    "transformation = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize(224)\n",
    "])\n",
    "\n",
    "# apply transformation to dataset\n",
    "for i in range(0, len(data_train)):\n",
    "    data_train[i][0].transform = transformation\n",
    "for i in range(0, len(data_test)):\n",
    "    data_test[i][0].transform = transformation\n",
    "\n",
    "# make dataloader\n",
    "train_dl = DataLoader(data_train, batch_size=10, shuffle=True)\n",
    "val_dl = DataLoader(data_test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 4, 224, 224)\n",
    "    model = Swish()\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())\n",
    "    \n",
    "    # SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels * r),\n",
    "            Swish(),\n",
    "            nn.Linear(in_channels * r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 56, 17, 17)\n",
    "    model = SEBlock(x.size(1))\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())\n",
    "    \n",
    "class MBConv(nn.Module):\n",
    "    expand = 6\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first MBConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 16, 24, 24)\n",
    "    model = MBConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])\n",
    "    \n",
    "class SepConv(nn.Module):\n",
    "    expand = 1\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first SepConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(4, 16, 24, 24)\n",
    "    model = SepConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    # stochastic depth check\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])\n",
    "    \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "\n",
    "        channels = [int(x*width) for x in channels]\n",
    "        repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "        # stochastic depth\n",
    "        if stochastic_depth:\n",
    "            self.p = p\n",
    "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "        else:\n",
    "            self.p = 1\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "        # efficient net\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(4, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(channels[8], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "        strides = [stride] + [1] * (repeats - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "            in_channels = out_channels\n",
    "            self.p -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b1(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b2(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b3(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b4(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b5(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b6(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "def efficientnet_b7(num_classes=3):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.randn(4, 4, 224, 224).to(device)\n",
    "    model = efficientnet_b4().to(device)\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05565d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# define loss function, optimizer, lr_scheduler\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "# get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# calculate the metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss_b = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item(), metric_b\n",
    "\n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights=params['path2weights']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3893b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':60,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# check the directory to save weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8acd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Label Data\n",
    "path_dir = './PPTD/2'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "real_data_test = [] # Tensor 로 변환된 No Label Data\n",
    "for data in file_list:\n",
    "    temp = np.load('./PPTD/2/' + data)\n",
    "    tempData = torch.Tensor(temp)\n",
    "    real_data_test.append(tempData)\n",
    "    \n",
    "\n",
    "data_loader = DataLoader(dataset = real_data_test, batch_size=40, shuffle = False, drop_last = True)\n",
    "network = model\n",
    "\n",
    "with torch.no_grad(): # Model 의 Back Propagation 을 막음\n",
    "    for img in data_loader:\n",
    "        img = img.to(device)\n",
    "        pred = network(img)\n",
    "        tmp = torch.softmax(pred, 1)\n",
    "\n",
    "        sumAD = 0\n",
    "        sumNM = 0\n",
    "        sumPD = 0\n",
    "        \n",
    "        for i in range(0, 40):\n",
    "            sumAD = sumAD + tmp[i][0]\n",
    "            sumNM = sumNM + tmp[i][1]\n",
    "            sumPD = sumPD + tmp[i][2]\n",
    "        \n",
    "        print(sumAD, sumNM, sumPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b375e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = model\n",
    "\n",
    "# val_ds train Data를 8 : 2 로 나눈 데이터중 validation Dataset\n",
    "data_loader = DataLoader(dataset = data_test, batch_size=40, shuffle = False, drop_last = True)\n",
    "\n",
    "# Label 된 Data를 통해 No Label Data의 기준을 잡음\n",
    "with torch.no_grad(): # Model 의 Back Propagation 을 막음\n",
    "    for img in data_loader:\n",
    "        img = img.to(device)\n",
    "        pred = network(img)\n",
    "        tmp = torch.softmax(pred, 1)\n",
    "\n",
    "        sumAD = 0\n",
    "        sumNM = 0\n",
    "        sumPD = 0\n",
    "        \n",
    "        for i in range(0, 40):\n",
    "            sumAD = sumAD + tmp[i][0]\n",
    "            sumNM = sumNM + tmp[i][1]\n",
    "            sumPD = sumPD + tmp[i][2]\n",
    "        \n",
    "        print(sumAD, sumNM, sumPD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
